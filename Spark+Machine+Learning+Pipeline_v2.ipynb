{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Machine Learning Pipeline\n",
    "\n",
    "This coursework is about implementing and applying Spark Machine Learning Pipelines, and evaluating them with respect to preprocessing, parametrisation, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Data set initial analysis and summary of pipeline task. (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1. Summary of machine learning pipeline\n",
    "Step 1.  \n",
    "Step 2.  \n",
    "Step 3.  \n",
    "Step 4.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2. Loading data to RDD and first preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "type_dict={'ncodpers':np.int32,\n",
    "           'ind_ahor_fin_ult1':np.uint8, 'ind_aval_fin_ult1':np.uint8, \n",
    "           'ind_cco_fin_ult1':np.uint8,'ind_cder_fin_ult1':np.uint8,\n",
    "           'ind_cno_fin_ult1':np.uint8,'ind_ctju_fin_ult1':np.uint8,'ind_ctma_fin_ult1':np.uint8,\n",
    "           'ind_ctop_fin_ult1':np.uint8,'ind_ctpp_fin_ult1':np.uint8,'ind_deco_fin_ult1':np.uint8,\n",
    "           'ind_deme_fin_ult1':np.uint8,'ind_dela_fin_ult1':np.uint8,'ind_ecue_fin_ult1':np.uint8,\n",
    "           'ind_fond_fin_ult1':np.uint8,'ind_hip_fin_ult1':np.uint8,'ind_plan_fin_ult1':np.uint8,\n",
    "           'ind_pres_fin_ult1':np.uint8,'ind_reca_fin_ult1':np.uint8,'ind_tjcr_fin_ult1':np.uint8,\n",
    "           'ind_valo_fin_ult1':np.uint8,'ind_viv_fin_ult1':np.uint8, 'ind_recibo_ult1':np.uint8 }\n",
    "\n",
    "# load data from server into dataframe (only loading the top 1,000,000 for demonstration purpose)\n",
    "df=pd.read_csv(\"/data/tempstore/santander-products/train_ver2.csv\", nrows=1000000, dtype=type_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep only unique id\n",
    "unique_ids = pd.Series(df[\"ncodpers\"].unique())\n",
    "df = df[df.ncodpers.isin(unique_ids)]  \n",
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eliminate mostly empty columns and redundant variables\n",
    "df.drop([\"tipodom\",\"cod_prov\", \"ult_fec_cli_1t\",\"conyuemp\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform to numeric and set missing values to nan\n",
    "df['age']=pd.to_numeric(df.age, errors='coerce')\n",
    "df['ind_nuevo']=pd.to_numeric(df.ind_nuevo, errors='coerce')\n",
    "df['antiguedad']=pd.to_numeric(df.antiguedad, errors='coerce')\n",
    "df['indrel']=pd.to_numeric(df.indrel, errors='coerce')\n",
    "df['renta']=pd.to_numeric(df.renta, errors='coerce')\n",
    "df['indrel_1mes']=pd.to_numeric(df.indrel_1mes, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove age outliers and nan from age variable\n",
    "df.loc[df.age < 18,\"age\"]  = df.loc[(df.age >= 18) & (df.age <= 30),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df.loc[df.age > 100,\"age\"] = df.loc[(df.age >= 30) & (df.age <= 100),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df[\"age\"].fillna(df[\"age\"].mean(),inplace=True) # replace nan with mean\n",
    "df[\"age\"] = df[\"age\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transfor dates to datetime datatype\n",
    "df[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_dato\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill datetime missing values\n",
    "dates=df.loc[:,\"fecha_alta\"].sort_values().reset_index()\n",
    "median_date = int(np.median(dates.index.values))\n",
    "df.loc[df.fecha_alta.isnull(),\"fecha_alta\"] = dates.loc[median_date,\"fecha_alta\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check all missing values\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values in target features with 0\n",
    "# target features = boolean indicator as to whether or not that product was owned that month\n",
    "df.loc[df.ind_nomina_ult1.isnull(), \"ind_nomina_ult1\"] = 0\n",
    "df.loc[df.ind_nom_pens_ult1.isnull(), \"ind_nom_pens_ult1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace other missing values\n",
    "df.loc[df[\"ind_nuevo\"].isnull(),\"ind_nuevo\"] = 1                   # new customers id '1'\n",
    "df.loc[df.antiguedad.isnull(),\"antiguedad\"] = df.antiguedad.min()\n",
    "df.loc[df.antiguedad <0, \"antiguedad\"] = 0                         # new customer antiguedad '0'\n",
    "df.loc[df.indrel.isnull(),\"indrel\"] = 1 \n",
    "df.loc[df.ind_actividad_cliente.isnull(),\"ind_actividad_cliente\"] = \\\n",
    "df[\"ind_actividad_cliente\"].median()                   # fill in customer activity missing\n",
    "df.loc[df.nomprov.isnull(),\"nomprov\"] = \"UNKNOWN\"      # known values for city of residence\n",
    "df.loc[df.indfall.isnull(),\"indfall\"] = \"N\"            # missing deceased index set to N\n",
    "df.loc[df.tiprel_1mes.isnull(),\"tiprel_1mes\"] = \"A\"    # customer status, if missing = active \n",
    "df.tiprel_1mes = df.tiprel_1mes.astype(\"category\")     # customer status as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Customer type normalization as categorical variable \n",
    "map_dict = { 1.0:\"1\", \"1.0\":\"1\", \"1\":\"1\", \"3.0\":\"3\", \"P\":\"P\", 3.0:\"3\", 2.0:\"2\", \"3\":\"3\", \"2.0\":\"2\", \"4.0\":\"4\", \"4\":\"4\", \"2\":\"2\"}\n",
    "df.indrel_1mes.fillna(\"P\",inplace=True)\n",
    "df.indrel_1mes = df.indrel_1mes.apply(lambda x: map_dict.get(x,x))\n",
    "df.indrel_1mes = df.indrel_1mes.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove rows with any nan value left\n",
    "df = df.dropna(subset=['renta', 'segmento', 'canal_entrada', 'ind_empleado', \n",
    "                       'pais_residencia', 'indresi', 'indresi', 'sexo'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check all missing values are gone\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Implementation of machine learning pipeline. (25%)\n",
    "Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. Test that your pipeline it is correctly implemented and explain your choice of processing steps, learning algorithms, and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# code modified from Spark documentation at:\n",
    "# https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forest-classifier\n",
    "# and DataBricks at:\n",
    "# https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "\n",
    "# imports dependencies for Random Forest pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# stages in the Pipeline\n",
    "stages = []\n",
    "\n",
    "# One-Hot Encoding\n",
    "categoricalColumns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"j\"]\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\") # Category Indexing with StringIndexer\n",
    "    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\") # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    stages += [stringIndexer, encoder]  # Add stages to the pipeline\n",
    "    \n",
    "# Convert labels into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"add here target column in csv file\", outputCol = \"labels\")\n",
    "stages += [label_stringIdx]  # Add stage to the pipeline\n",
    "\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"m\", \"n\", \"o\", \"p\", \"q\", \"r\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]  # Add stage to the pipeline\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"labels\", \n",
    "                            featuresCol=\"features\", \n",
    "                            numTrees=100,                 #  Number of trees in the random forest\n",
    "                            impurity='entropy',            # Criterion used for information gain calculation\n",
    "                            featureSubsetStrategy=\"auto\",\n",
    "                            predictionCol=\"prediction\"\n",
    "                            maxDepth=5, \n",
    "                            maxBins=32, \n",
    "                            minInstancesPerNode=1, \n",
    "                            minInfoGain=0.0, \n",
    "                            subsamplingRate=1.0)\n",
    "stages += [rf]  # Add stage to the pipeline\n",
    "\n",
    "# Machine Learning Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Evaluation and test of model. (20%)\n",
    "Evaluate the performance of your pipeline using training and test set (donâ€™t use CV but pyspark.ml.tuning.TrainValidationSplit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1. Evaluate performance of machine learning pipeline on training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports dependencies\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Split data into training set and testing set\n",
    "[trainData, testData] = trainData.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "# Train model in pipeline\n",
    "rfModel = pipeline.fit(trainData)\n",
    "\n",
    "# Make predictions for training set and compute training set accuracy\n",
    "predictions = rfModel.transform(trainData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labels\", \n",
    "                                              predictionCol=\"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(train_pipeline.stages[0])  # summary\n",
    "\n",
    "\n",
    "# Run the feature transformations pipeline on the test data set\n",
    "pipelineModel = prePro_pipeline.fit(testClients)  #  computes feature statistics\n",
    "testData = pipelineModel.transform(testClients)  #  transforms the features\n",
    "\n",
    "# Make predictions for test set and compute test error\n",
    "test_predictions = rfModel.transform(testData)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Model fine-tuning. (35%) \n",
    "Implement a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. Document the training and test performance and the time taken for training and testing. Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1. Training set size evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Training set size evaluation')\n",
    "\n",
    "# size of different training set to be evaluated, and split of training set\n",
    "sizes = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "data = trainData.randomSplit(sizes, seed = 100)\n",
    "\n",
    "print('\\n=== training set of size 100%')\n",
    "# Train model in pipeline\n",
    "tempModel = pipeline.fit(trainData)\n",
    "# Make predictions for training set and compute training set accuracy\n",
    "tempPredictions = tempModel.transform(trainData)\n",
    "tempAccuracy = evaluator.evaluate(tempPredictions)\n",
    "print(\"Classification Error = %g\" % (1.0 - tempAccuracy))\n",
    "\n",
    "for x in data:\n",
    "    print('\\n=== training set of size reduced to %g' % x)\n",
    "    # Train model in pipeline\n",
    "    tempModel = pipeline.fit(data[x])\n",
    "    # Make predictions for training set and compute training set accuracy\n",
    "    tempPredictions = tempModel.transform(data[x])\n",
    "    tempAccuracy = evaluator.evaluate(tempPredictions)\n",
    "    print(\"Classification Error = %g\" % (1.0 - tempAccuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2. Machine Learning Model Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and their values to search and evaluate\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10,20,50,100,200,500,1000,5000]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [0,1,2,4,6,8,10]) \\\n",
    "    .addGrid(rf.maxDepth, [2,5,10,20,50]).build()\n",
    "\n",
    "# Grid Search and Cross Validation\n",
    "crossVal = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
    "print('starting Hyperparameter Grid Search with cross-validation')\n",
    "rfCrosVal = crossVal.fit(trainData)\n",
    "print('Grid Search has finished')\n",
    "\n",
    "print(rfCrosVal.bestModel.rank)\n",
    "paramMap = list(zip(rfCrosVal.getEstimatorParamMaps(),rfCrosVal.avgMetrics))\n",
    "paramMax = max(paramMap, key=lambda x: x[1])\n",
    "print(paramMax)\n",
    "\n",
    "# Evaluate the model with test data\n",
    "cvtest_predictions = rfCrosVal.transform(testData)\n",
    "cvtest_accuracy = evaluator.evaluate(cvtest_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - cvtest_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3. Evaluate model performance using a subset of variables (predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
