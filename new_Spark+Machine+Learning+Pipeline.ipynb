{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Machine Learning Pipeline\n",
    "\n",
    "This coursework is about implementing and applying Spark Machine Learning Pipelines, and evaluating them with respect to preprocessing, parametrisation, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Data set initial analysis and summary of pipeline task. (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1. Summary of machine learning pipeline\n",
    "Step 1.  \n",
    "Step 2.  \n",
    "Step 3.  \n",
    "Step 4.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2. Loading data to RDD and first preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependencies for creating a data frame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(fecha_dato=datetime.date(2015, 1, 28), ncodpers=1375586.0, ind_empleado='N', pais_residencia='ES', sexo='H', age=35.0, fecha_alta=datetime.date(2015, 1, 12), ind_nuevo=0.0, antiguedad=6.0, indrel=1.0, indrel_1mes=1.0, tiprel_1mes='A', indresi='S', indext='N', canal_entrada='KHL', indfall='N', tipodom=1.0, cod_prov=29.0, nomprov='MALAGA', ind_actividad_cliente=1.0, renta=87218.1015625, segmento='02 - PARTICULARES', ind_ahor_fin_ult1=0.0, ind_aval_fin_ult1=0.0, ind_cco_fin_ult1=1.0, ind_cder_fin_ult1=0.0, ind_cno_fin_ult1=0.0, ind_ctju_fin_ult1=0.0, ind_ctma_fin_ult1=0.0, ind_ctop_fin_ult1=0.0, ind_ctpp_fin_ult1=0.0, ind_deco_fin_ult1=0.0, ind_deme_fin_ult1=0.0, ind_dela_fin_ult1=0.0, ind_ecue_fin_ult1=0.0, ind_fond_fin_ult1=0.0, ind_hip_fin_ult1=0.0, ind_plan_fin_ult1=0.0, ind_pres_fin_ult1=0.0, ind_reca_fin_ult1=0.0, ind_tjcr_fin_ult1=0.0, ind_valo_fin_ult1=0.0, ind_viv_fin_ult1=0.0, ind_nomina_ult1=0.0, ind_nom_pens_ult1=0.0, ind_recibo_ult1=0.0),\n",
       " Row(fecha_dato=datetime.date(2015, 1, 28), ncodpers=1050611.0, ind_empleado='N', pais_residencia='ES', sexo='V', age=23.0, fecha_alta=datetime.date(2012, 8, 10), ind_nuevo=0.0, antiguedad=35.0, indrel=1.0, indrel_1mes=1.0, tiprel_1mes='I', indresi='S', indext='S', canal_entrada='KHE', indfall='N', tipodom=1.0, cod_prov=13.0, nomprov='CIUDAD REAL', ind_actividad_cliente=0.0, renta=35548.73828125, segmento='03 - UNIVERSITARIO', ind_ahor_fin_ult1=0.0, ind_aval_fin_ult1=0.0, ind_cco_fin_ult1=1.0, ind_cder_fin_ult1=0.0, ind_cno_fin_ult1=0.0, ind_ctju_fin_ult1=0.0, ind_ctma_fin_ult1=0.0, ind_ctop_fin_ult1=0.0, ind_ctpp_fin_ult1=0.0, ind_deco_fin_ult1=0.0, ind_deme_fin_ult1=0.0, ind_dela_fin_ult1=0.0, ind_ecue_fin_ult1=0.0, ind_fond_fin_ult1=0.0, ind_hip_fin_ult1=0.0, ind_plan_fin_ult1=0.0, ind_pres_fin_ult1=0.0, ind_reca_fin_ult1=0.0, ind_tjcr_fin_ult1=0.0, ind_valo_fin_ult1=0.0, ind_viv_fin_ult1=0.0, ind_nomina_ult1=0.0, ind_nom_pens_ult1=0.0, ind_recibo_ult1=0.0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATING DATA FRAME  \n",
    "        # Defining column data type by \"cast\" command\n",
    "        # deleting empty columns by \"select\" command\n",
    "        \n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "\n",
    "SantanderRDD = spark.read.csv(\"/data/tempstore/santander-products/train_ver2.csv\", \n",
    "                                  header=True,\n",
    "                                  mode=\"DROPMALFORMED\")\n",
    "\n",
    "SantanderRDD = SantanderRDD.select(SantanderRDD.fecha_dato.cast(\"date\"),\n",
    "                                   SantanderRDD.ncodpers.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_empleado.cast(\"string\"),\n",
    "                                   SantanderRDD.pais_residencia.cast(\"string\"),\n",
    "                                   SantanderRDD.sexo.cast(\"string\"),\n",
    "                                   SantanderRDD.age.cast(\"float\"),\n",
    "                                   SantanderRDD.fecha_alta.cast(\"date\"),\n",
    "                                   SantanderRDD.ind_nuevo.cast(\"float\"),\n",
    "                                   SantanderRDD.antiguedad.cast(\"float\"),\n",
    "                                   SantanderRDD.indrel.cast(\"float\"),\n",
    "                                   #SantanderRDD.ult_fec_cli_1t.cast(\"date\"), Este fuera!\n",
    "                                   SantanderRDD.indrel_1mes.cast(\"float\"),\n",
    "                                   SantanderRDD.tiprel_1mes.cast(\"string\"),\n",
    "                                   SantanderRDD.indresi.cast(\"string\"),\n",
    "                                   SantanderRDD.indext.cast(\"string\"),\n",
    "                                   #SantanderRDD.conyuemp.cast(\"string\"), fuera!\n",
    "                                   SantanderRDD.canal_entrada.cast(\"string\"),\n",
    "                                   SantanderRDD.indfall.cast(\"string\"),\n",
    "                                   SantanderRDD.tipodom.cast(\"float\"),\n",
    "                                   SantanderRDD.cod_prov.cast(\"float\"),\n",
    "                                   SantanderRDD.nomprov.cast(\"string\"),\n",
    "                                   SantanderRDD.ind_actividad_cliente.cast(\"float\"),\n",
    "                                   SantanderRDD.renta.cast(\"float\"),\n",
    "                                   SantanderRDD.segmento.cast(\"string\"),\n",
    "                                   SantanderRDD.ind_ahor_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_aval_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_cco_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_cder_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_cno_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_ctju_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_ctma_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_ctop_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_ctpp_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_deco_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_deme_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_dela_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_ecue_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_fond_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_hip_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_plan_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_pres_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_reca_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_tjcr_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_valo_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_viv_fin_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_nomina_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_nom_pens_ult1.cast(\"float\"),\n",
    "                                   SantanderRDD.ind_recibo_ult1.cast(\"float\"))\n",
    "\n",
    "## TESTING\n",
    "SantanderRDD.take(2)\n",
    "#SantanderRDD.show()\n",
    "#SantanderRDD.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'hdfs://saltdean/data/data/santander-products/train_ver2.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6fd4e666454d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlimit_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m df = pd.read_csv(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\",\n\u001b[0;32m---> 11\u001b[0;31m                            dtype={\"sexo\":str, \"ult_fec_cli_1t\":str, \"indext\":str}, nrows=limit_rows)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0munique_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ncodpers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8440)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'hdfs://saltdean/data/data/santander-products/train_ver2.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Code modified from \n",
    "# https://www.kaggle.com/apryor6/santander-product-recommendation/detailed-cleaning-visualization-python/notebook\n",
    "\n",
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# create dataframe 'df'\n",
    "limit_rows = 7000000\n",
    "df = pd.read_csv(\"hdfs://saltdean/data/data/santander-products/train_ver2.csv\",\n",
    "                           dtype={\"sexo\":str, \"ult_fec_cli_1t\":str, \"indext\":str}, nrows=limit_rows)\n",
    "\n",
    "unique_ids = pd.Series(df[\"ncodpers\"].unique())\n",
    "limit_people = 1.2e4\n",
    "unique_id = unique_ids.sample(n=limit_people)\n",
    "df = df[df.ncodpers.isin(unique_id)]\n",
    "df.count()     # number of instances\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# find missing values\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove age outliers and nan from dataframe\n",
    "df.loc[df.age < 18,\"age\"]  = df.loc[(df.age >= 18) & (df.age <= 30),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df.loc[df.age > 100,\"age\"] = df.loc[(df.age >= 30) & (df.age <= 100),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df[\"age\"].fillna(df[\"age\"].mean(),inplace=True) # replace nan with mean\n",
    "df[\"age\"] = df[\"age\"].astype(int)\n",
    "\n",
    "# Replace missing values\n",
    "df.loc[df[\"ind_nuevo\"].isnull(),\"ind_nuevo\"] = 1                   # new customers id '1'\n",
    "df.loc[df.antiguedad.isnull(),\"antiguedad\"] = df.antiguedad.min()\n",
    "df.loc[df.antiguedad <0, \"antiguedad\"] = 0                         # new customer antiguedad '0'\n",
    "df.loc[df.indrel.isnull(),\"indrel\"] = 1 \n",
    "\n",
    "dates=df.loc[:,\"fecha_alta\"].sort_values().reset_index()\n",
    "median_date = int(np.median(dates.index.values))\n",
    "df.loc[df.fecha_alta.isnull(),\"fecha_alta\"] = dates.loc[median_date,\"fecha_alta\"] # fill join date missing values\n",
    "\n",
    "df.loc[df.ind_actividad_cliente.isnull(),\"ind_actividad_cliente\"] = \\\n",
    "df[\"ind_actividad_cliente\"].median()                   # fill in customer activity missing\n",
    "\n",
    "df.loc[df.nomprov.isnull(),\"nomprov\"] = \"UNKNOWN\"      # known values for city of residence\n",
    "\n",
    "df.loc[df.indfall.isnull(),\"indfall\"] = \"N\"            # missing deceased index set to N\n",
    "df.loc[df.tiprel_1mes.isnull(),\"tiprel_1mes\"] = \"A\"    # customer status, if missing = active \n",
    "df.tiprel_1mes = df.tiprel_1mes.astype(\"category\")     # customer status as categorical\n",
    "\n",
    "# Customer type normalization as categorical variable \n",
    "map_dict = { 1.0:\"1\", \"1.0\":\"1\", \"1\":\"1\", \"3.0\":\"3\", \"P\":\"P\", 3.0:\"3\", 2.0:\"2\", \"3\":\"3\", \"2.0\":\"2\", \"4.0\":\"4\", \"4\":\"4\", \"2\":\"2\"}\n",
    "df.indrel_1mes.fillna(\"P\",inplace=True)\n",
    "df.indrel_1mes = df.indrel_1mes.apply(lambda x: map_dict.get(x,x))\n",
    "df.indrel_1mes = df.indrel_1mes.astype(\"category\")\n",
    "\n",
    "# Replace missing values in target features with 0\n",
    "# target features = boolean indicator as to whether or not that product was owned that month\n",
    "df.loc[df.ind_nomina_ult1.isnull(), \"ind_nomina_ult1\"] = 0\n",
    "df.loc[df.ind_nom_pens_ult1.isnull(), \"ind_nom_pens_ult1\"] = 0\n",
    "\n",
    "# Elimnate entries with nan values in given variable, eg:\n",
    "print(\"Total number of entries before removing nan= \", df.count())\n",
    "df.renta.isnull().sum()\n",
    "df.na.drop(subset=[\"renta\",\"indfall\",\"tiprel_1mes\",\"indrel_1mes\"]) # !!!! need to be tested that only nan entries are removed\n",
    "df.renta.isnull().sum()\n",
    "print(\"Total number of entries after removing nan= \", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Eliminate redundant variables\n",
    "df.drop([\"tipodom\",\"cod_prov\"],axis=1,inplace=True)\n",
    "\n",
    "# check all missing values are gone\n",
    "df.isnull().any()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert target features column into integers\n",
    "feature_cols = df.iloc[:1,].filter(regex=\"ind_+.*ult.*\").columns.values\n",
    "for col in feature_cols:\n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Implementation of machine learning pipeline. (25%)\n",
    "Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. Test that your pipeline it is correctly implemented and explain your choice of processing steps, learning algorithms, and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# code modified from Spark documentation at:\n",
    "# https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forest-classifier\n",
    "# and DataBricks at:\n",
    "# https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "\n",
    "# imports dependencies for Random Forest pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# stages in the Pipeline\n",
    "stages = []\n",
    "\n",
    "# One-Hot Encoding\n",
    "categoricalColumns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"j\"]\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\") # Category Indexing with StringIndexer\n",
    "    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\") # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    stages += [stringIndexer, encoder]  # Add stages to the pipeline\n",
    "    \n",
    "# Convert labels into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"add here target column in csv file\", outputCol = \"labels\")\n",
    "stages += [label_stringIdx]  # Add stage to the pipeline\n",
    "\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"m\", \"n\", \"o\", \"p\", \"q\", \"r\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]  # Add stage to the pipeline\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"labels\", \n",
    "                            featuresCol=\"features\", \n",
    "                            numTrees=100,                 #  Number of trees in the random forest\n",
    "                            impurity='entropy',            # Criterion used for information gain calculation\n",
    "                            featureSubsetStrategy=\"auto\",\n",
    "                            predictionCol=\"prediction\"\n",
    "                            maxDepth=5, \n",
    "                            maxBins=32, \n",
    "                            minInstancesPerNode=1, \n",
    "                            minInfoGain=0.0, \n",
    "                            subsamplingRate=1.0)\n",
    "stages += [rf]  # Add stage to the pipeline\n",
    "\n",
    "# Machine Learning Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Evaluation and test of model. (20%)\n",
    "Evaluate the performance of your pipeline using training and test set (donâ€™t use CV but pyspark.ml.tuning.TrainValidationSplit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1. Evaluate performance of machine learning pipeline on training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports dependencies\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Split data into training set and testing set\n",
    "[trainData, testData] = trainData.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "# Train model in pipeline\n",
    "rfModel = pipeline.fit(trainData)\n",
    "\n",
    "# Make predictions for training set and compute training set accuracy\n",
    "predictions = rfModel.transform(trainData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labels\", \n",
    "                                              predictionCol=\"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(train_pipeline.stages[0])  # summary\n",
    "\n",
    "\n",
    "# Run the feature transformations pipeline on the test data set\n",
    "pipelineModel = prePro_pipeline.fit(testClients)  #  computes feature statistics\n",
    "testData = pipelineModel.transform(testClients)  #  transforms the features\n",
    "\n",
    "# Make predictions for test set and compute test error\n",
    "test_predictions = rfModel.transform(testData)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Model fine-tuning. (35%) \n",
    "Implement a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. Document the training and test performance and the time taken for training and testing. Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1. Training set size evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Training set size evaluation')\n",
    "\n",
    "# size of different training set to be evaluated, and split of training set\n",
    "sizes = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "data = trainData.randomSplit(sizes, seed = 100)\n",
    "\n",
    "print('\\n=== training set of size 100%')\n",
    "# Train model in pipeline\n",
    "tempModel = pipeline.fit(trainData)\n",
    "# Make predictions for training set and compute training set accuracy\n",
    "tempPredictions = tempModel.transform(trainData)\n",
    "tempAccuracy = evaluator.evaluate(tempPredictions)\n",
    "print(\"Classification Error = %g\" % (1.0 - tempAccuracy))\n",
    "\n",
    "for x in data:\n",
    "    print('\\n=== training set of size reduced to %g' % x)\n",
    "    # Train model in pipeline\n",
    "    tempModel = pipeline.fit(data[x])\n",
    "    # Make predictions for training set and compute training set accuracy\n",
    "    tempPredictions = tempModel.transform(data[x])\n",
    "    tempAccuracy = evaluator.evaluate(tempPredictions)\n",
    "    print(\"Classification Error = %g\" % (1.0 - tempAccuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2. Machine Learning Model Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and their values to search and evaluate\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10,20,50,100,200,500,1000,5000]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [0,1,2,4,6,8,10]) \\\n",
    "    .addGrid(rf.maxDepth, [2,5,10,20,50]).build()\n",
    "\n",
    "# Grid Search and Cross Validation\n",
    "crossVal = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
    "print('starting Hyperparameter Grid Search with cross-validation')\n",
    "rfCrosVal = crossVal.fit(trainData)\n",
    "print('Grid Search has finished')\n",
    "\n",
    "print(rfCrosVal.bestModel.rank)\n",
    "paramMap = list(zip(rfCrosVal.getEstimatorParamMaps(),rfCrosVal.avgMetrics))\n",
    "paramMax = max(paramMap, key=lambda x: x[1])\n",
    "print(paramMax)\n",
    "\n",
    "# Evaluate the model with test data\n",
    "cvtest_predictions = rfCrosVal.transform(testData)\n",
    "cvtest_accuracy = evaluator.evaluate(cvtest_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - cvtest_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3. Evaluate model performance using a subset of variables (predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
